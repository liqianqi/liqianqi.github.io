---
title: 性能优化
author: liqianqi
date: 2023-12-31 20:40:00 +0800
categories: [RM]
tags: []
math: false
mermaid: false
pin: false
image:
  src: 
  width: 600
  height: 337
---

# 性能优化

## cpu-bound 与 内存-bound
- 通常来说，并行加速只能加速计算部分，不能加速内存读写部分，因此并行计算经常遇到内存瓶颈。
- 因为一次浮点加法的计算量和访存的超高延迟相比实在太少了。
- 经验公式: 1次浮点读写 ≈ 8次浮点加法
- 如果矢量化成功（SSE）：1次浮点读写 ≈ 32次浮点加法
- 如果CPU有4核且矢量化成功：1次浮点读写 ≈ 128次浮点加法<br>
所以就要足够的计算量来隐藏操作的延迟。
- 超线程技术: 不一定是两个线程在两个核上同时运行，而是有可能两个线程同时运行在一个核上，由硬件自动来调度。这个主要是针对如果内存卡住，cpu会自动切换到另一个核上。
    ```cpp
    #include <iostream>
    #include <vector>
    #include <cmath>
    #include <cstring>
    #include <cstdlib>
    #include <array>
    #include <benchmark/benchmark.h>
    #include <x86intrin.h>
    #include <omp.h>

    constexpr size_t n = 1<<26;

    std::vector<float> a(n);  // 256MB

    void BM_fill(benchmark::State &bm) {
        for (auto _: bm) {
            for (size_t i = 0; i < n; i++) {
                a[i] = 1;
            }
        }
    }
    BENCHMARK(BM_fill);

    void BM_parallel_fill(benchmark::State &bm) {
        for (auto _: bm) {
    #pragma omp parallel for
            for (size_t i = 0; i < n; i++) {
                a[i] = 1;
            }
        }
    }
    BENCHMARK(BM_parallel_fill);

    void BM_sine(benchmark::State &bm) {
        for (auto _: bm) {
            for (size_t i = 0; i < n; i++) {
                a[i] = std::sin(i);
            }
        }
    }
    BENCHMARK(BM_sine);

    void BM_parallel_sine(benchmark::State &bm) {
        for (auto _: bm) {
    #pragma omp parallel for
            for (size_t i = 0; i < n; i++) {
                a[i] = std::sin(i);
            }
        }
    }
    BENCHMARK(BM_parallel_sine);

    BENCHMARK_MAIN();
    ```
- 这是运行结果: <br>
![image-20210825174125342](https://github.com/liqianqi/liqianqi.github.io/blob/master/_posts/2023-12-31-prefetch.assets/result.png)
<!-- <img src="https://github.com/liqianqi/liqianqi.github.io/blob/master/_posts/2023-12-31-prefetch.assets/result.png" alt="IMG_4627" style="zoom:30%;" /> -->
- 我们发现，a[i]=1和a[i]=a[i]+1时间差不多，说明访存时间远大于加时间，当计算复杂度高到一定程度后，并行的优势才体现出来。

## CPU中的高速缓存
![image-20210825174125342](https://github.com/liqianqi/liqianqi.github.io/blob/master/_posts/2023-12-31-prefetch.assets/cache.png)
<!-- <img src="https://github.com/liqianqi/liqianqi.github.io/blob/master/_posts/2023-12-31-prefetch.assets/cache.png" alt="IMG_4627" style="zoom:30%;" /> <br> -->
- 这是CPU三个缓存的分级结构图, L1,L2缓存是只给自己核心用的，而L3比较大，可以给多个核心使用。
- 数据小到装的进二级缓存，则最大带宽就取决于二级缓存的带宽。稍微大一点则只能装到三级缓存，就取决于三级缓存的带宽。三级缓存也装不下，那就取决于主内存的带宽了。<br>
- 结论：要避免mem-bound，数据量尽量足够小，如果能装的进缓存就高效了。

### 缓存的读取机制
- 通俗点来讲，当CPU想要读取一个地址的时候,就会跟缓存说，我要读取这个地址，缓存就去查找，看看这个地址有没有存储过，要是存储过就直接将缓存里存的数据返回给CPU，要是没找到，就像下一级缓存下令，让它去读，如果三级缓存也读不到，三级缓存就会向主内存发送请求，就会创建一个新条目，这样下一次再寻找这个数据的时候就不用再去主内存里读取了。

- 在X86架构中，这个条目的大小是64字节。比如当访问 0x0048~0x0050 这 4 个字节时，实际会导致 0x0040~0x0080 的 64 字节数据整个被读取到缓存中。这64字节叫缓存行，一个读取单位。(数据结构对齐到缓存行)

- 缓存的数据结构: <br>
    ```cpp
    struct CacheEntry
    {
        bool valid;
        uint64_t address;
        char data[64];
    };
    CacheEntry cache[512];
    ```

### 缓存的写入机制
- 当CPU写入一个数组时，缓存会查找与该地址匹配的条目，如果找到，那就修改数据，如果没有找到，就创建一个新条目，并且标记为dirty。

- 当读和写创建的新条目过多，缓存快要塞不下时，他会把最不常用的那个条目移除，这个现象称为失效（invalid）。如果那个条目时刚刚读的时候创建的，那没问题可以删，但是如果那个条目是被标记为脏的，则说明是当时打算写入的数据，那就麻烦了，需要向主内存发送写入请求，等他写入成功，才能安全移除这个条目。

- 如有多级缓存，则一级缓存失效后会丢给二级缓存。二级再丢给三级，三级最后丢给主内存。

- 如果访问数组时，按一定的间距跨步访问，则效率如何？

- 从1到16都是一样快的，32开始才按2的倍率变快，为什么？

- 因为CPU和内存之间隔着缓存，而缓存和内存之间传输数据的最小单位是缓存行（64字节）。16个float是64字节，所以小于64字节的跨步访问，都会导致数据全部被读取出来。而超过64字节的跨步，则中间的缓存行没有被读取，从而变快了。

## AOS与SOA和AOSOA
```cpp
struct a
{
    int x;
    int y;
    int z;
};
a array[1024];  // AOS
```

```cpp
struct a
{
    int x[1024];
    int y[1024];
    int z[1024];
};
a array;        // SOA
```

```cpp
struct mayclass
{
    int x[1024];
    int y[1024];
    int z[1024];
}
std::vector<myclass> mc(n/1024);// AOSOA, 要保证mc.size()是1024整数倍。
```
- 如果只是访问其中一个属性，那么SOA优于AOS，可以保证缓存行的不浪费
- 如果都要访问，那么AOS优于SOA，这是因为缓存预取机制维护维护的长度是一样的，赛道越多，每个赛道长度越短，prefetch机制优势无法显现(cpu能够预测下一步骤的长度短，假设数组是连续的，一个赛道能提前预测16个，4个赛道能提前预测4个，延迟隐藏不明显)，这个长度就是缓存容量。